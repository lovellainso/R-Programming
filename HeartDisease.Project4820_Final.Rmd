---
title: "Project DANA 4820"
date: "2023-03-30"
output: pdf_document
---

  GROUP MEMBERS:
       "Lovella Inso 100392634"
       "Yamileth Hercules 100385215", 
        "Mary Alexandra Garcia 100391387", 
        "Maria Christina Alexandria Perocho 100392385", 
        


Purpose:
The principal purpose of this project will be on data cleaning, merging different data sets, aggregating variables, and exploring the variables to see association between the categorical variables and correlation between the numerical ones. The data set that we are going to explore is about Heart Disease. 

The use of databases and data analysis techniques have become increasingly important in the field of medicine, particularly in the study and treatment of diseases such as heart disease. By analyzing large amounts of medical data, researchers and healthcare professionals can gain valuable insights into the causes, risk factors, and potential treatments for these conditions.Additionally, the use of databases and data analysis can improve patient outcomes by providing more personalized and effective care. As such, a comprehensive analysis of medical databases is crucial for advancing our understanding of heart disease and developing better strategies for prevention and treatment.

## 1.	Introduce your data: Heart Disease Database

#a. What is the population of interest:

The population of interest is the individuals that is having heart disease or at risk of developing heart disease. The Heart Disease Data Set consists of four separate databases: Cleveland, Hungary, Switzerland, and the VA Long Beach. Each database contains information about patients with heart disease, including demographic, behavioral, and medical factors.

Number of Instances:

 Cleveland: 303
 Hungarian: 294
 Switzerland: 123
 Long Beach VA: 200

# b.	What kind of variables do you have:

Attribute Information with description below:
  - (age): age in years
  - (sex): gender (1 = male; 0 = female)
  - (cp): chest pain type (1: typical angina, 2: atypical angina, 3: non-anginal pain, 4: asymptomatic)
  - (trestbps): resting blood pressure (in mm Hg on admission to the hospital)
  - (chol): serum cholesterol in mg/dl
  - (fbs): fasting blood sugar > 120 mg/dl (1 = true; 0 = false)       
  - (restecg): resting electrocardiographic results (0: normal, 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), 2: showing probable or definite left ventricular hypertrophy criteria).
  - (thalach): maximum heart rate achieved.
  - (exang): exercise induced angina (1 = yes; 0 = no)
  - (oldpeak): ST depression induced by exercise relative to rest.
  - (slope): the slope of the peak exercise ST segment (1: upsloping, 2: flat, 3: down sloping)   
  - (ca): number of major vessels (0-3) colored by flouroscopy
  - (thal): 3 = normal; 6 = fixed defect; 7 = reversible defect  
  - (HD): = (the predicted attribute) diagnosis of heart disease (0-without heart disease, 1-with heart disease)

# c.	Specify the type of variables:

The type of variables for this data set are as follows:

   Categorical Variables:           	Numerical Variables:
    sex: Nominal variable	            age: Continuous variable
    cp: Nominal variable            	trestbps: Continuous variable
    fbs: Binary variable             	chol: Continuous variable
    restecg: Nominal variable       	thalach: Continuous variable 
    exang: Binary variable          	oldpeak: Continuous variable
    slope: Nominal variable	
    thal: Nominal variable	
    HD: Binary variable	

## 2.	Data Cleaning and Manipulation:

# a.	Explain any data cleaning:

   -	We merged 4 databases because there are different locations where the data was collected with the use of pd.concat function.
   -	We renamed the columns with the corresponding names.
   -	For the following variables, there are 0 values which are not making sense because there is no value for 0, We replaced those values with NaN:
      •	trestbps(resting blood pressure)
      •	chol(serum cholesterol)
      •	fbs(fasting blood sugar)
      •	restecg(resting electrocardiographic results)
      •	thalach(maximum heart rate)
      •	exang(exercised induced angina)
      •	oldpeak(ST depression induced by exercise)
      •	slope(slope of the peak exercise)
      •	ca(number of major vessels)
      •	thal(diagnosis of heart disease)

  -	There are also some variables with a negative values which do not make sense.
     •	Trestbps (resting blood pressure)
     •	Chol (cholesterol serum)

  -	Some variables have one row with a missing value, we dropped that row.
     (Age, Sex, Cp, Num, Restecg)
  -	We replaced the NaN values with mean for the numerical variables such as trestbps, chol, thalach, and oldpeak.
  -	We replaced the NaN values with mode for the categorical variable such as fbs, exang, slope, ca, and thal.
  -	We transformed 'sex','cp', 'fbs', 'exang', 'slope', 'thal', 'restecg', 'num', 'ca' to category type.
  
# b. Explain any data manipulation (changing structure, creating new variables,...)
  -	We renamed variable num to HD which is the diagnosis for heart disease.
  - We modified the next numerical variables to categorical in order for R to read them as factors: sex, cp, fbs, restecg, exang, slope, ca, thal.

## 3.	Research Question:
# a.	What is the question: 
   Which of these variables are effective factors in determining the risk of getting heart disease?
# b.	Why is this interesting: 
   This data set is interesting because we can use these data to predict what are these factors that could possibly affect of having risk of getting heart disease.
# c.	Why logistic regression: 
   We use logistic regression in predictive model since the response variable is HD that is having binary values of 0- for not having heart disease and 1 – having heart disease.

# 4. Initial Selection of the variables:

Importing Needed Libraries
```{r}
library(pROC)
library(randomForest)
library(tidyverse)
library(nnet)
library(ROCR)
library(packHV)
library(plyr)
library(ggplot2)
library("car")
library("ggpubr")
library(interactions)
library(glmtoolbox)
library(car)
library(ResourceSelection)
```


```{r}
df<- read.csv('data/heart_disease.csv')
View(df)
head(df)
```
## 
```{r}
str(df)
```

### Declaring categorical variables as factor

```{r}
df$sex<-as.factor(df$sex)
df$cp<-as.factor(df$cp)
df$cp<-as.factor(df$cp)
df$fbs<-as.factor(df$fbs)
df$restecg<- as.factor(df$restecg)
df$exang<- as.factor(df$exang)
df$slope<-as.factor(df$slope)
df$ca<-as.factor(df$ca)
df$thal<- as.factor(df$thal)
df$HD<-as.factor(df$HD)
```

### Separating numerical variables 
```{r}
num_vars <- sapply(df, is.numeric)
num_df <- df[, num_vars]
head(num_df)
```

# Creating one subset for categorical variables
```{r}
sub <- df[, c("HD", "sex", "cp", "fbs", "restecg", "exang", "slope", "ca", "thal")]
sub
```
# >>> ANALYSIS OF CATEGORICAL VARIABLES

# Creating a barplot for each categorical variable to see the frequency:
```{r}
for (var in names(sub)) {
  table_cat <- table(sub[[var]])
  barplot(table_cat,col="lightblue", xlab = var,main = paste("Frequency of", var))
} 
```
### Interpretation: 
The variable sex shows that most of the patients in the study are males.
The variable cp shows most of the patients have chest pain asymptomatic.
The variable fbs shows that the majority of the patients do not have diabetes.
The variable restec shows that most of the patients have a normal resting electrocardiographic result.
The variable exang shows that most of the patients feel angina after exercise.
The variable slope shows that most of the patients did not have a significant change in the ST segment during exercise.
The variable ca shows that most of the patients do not present significant blockage or narrowing in any of the major coronary arteries.
The variable thal shows that most of the patients had normal blood flow to the heart muscle.
The variable HD shows that most of the patients have had heart disease.

# Creating a barplot for each categorical by each group of our response variable HD:

```{r}
for (var in names(sub)) {
  table_cat2 <- table(df$HD, sub[[var]])
  barplot(table_cat2,col= c("lightblue","red"), xlab = var,ylab = 'Frequency', main = paste("Frequency grouped of", var), beside= TRUE)
  legend("topright", legend = levels(df$HD), fill = c("lightblue", "red"))
} 
```
Based on the difference between the levels in each categorical variables for the groups in the HD variables, we can see that the categorical variables are not independent to the response variables, following we will test this assumption performing the chi.squared test:

## Performing the chi-squared to validate the independence between the response variable and the explanatory variables:

Creating the hypothesis:
Ho: The explanatory variable and the response variable 'HD' are independent to each other.
Ha: The explanatory variable and the response variable 'HD' are not independent to each other.

```{r}
combos <- combn(ncol(sub), 2)

df_list <- alply(combos, 2, function(x) {
  test <- chisq.test(sub[, x[1]], sub[, x[2]])
  
out <- data.frame('row' = colnames(sub)[x[1]]
                    , 'column' = colnames(sub[x[2]])
                    , "Chi.Square" = round(test$statistic,3)
                    ,  "df"= test$parameter
                    ,  "p.value" = test$p.value
                    )
  return(out)

})

result_df <- do.call(rbind, df_list)
head(result_df)
```

Since the p value is less than 0.05, we have enough statistical evidence to reject the null hypothesis and we can conclude that the explanatory variable and the response variable 'HD' are not independent to each other

# >>> ANALYSIS OF NUMERICAL VARIABLES

# Plotting the histogram and boxplot to see the distribution of numerical variables
```{r}
for (var in names(num_df)) {
  hist_boxplot(num_df[[var]],col="lightblue",freq=TRUE,  xlab = var,main = paste("Histogram of", var))
} 
```

### Interpretation: 
We can see the distribution and the boxplot of the variables in the above graphs. Now we are going to mention son interpretation of those:
The variable `age` show a normal distribution with no outliers. 
The variable `thalach` follows a normal distribution but we can see some outliers in the boxplot.
The variables that show a skewed distribution and have outliers are : `trestbps`, `chol` and  `oldpeak`. 

##### Note: 
In clinical studies, it is generally not recommended to filter or change the distribution of the data, including removing outliers, because doing so can introduce bias and distort the results of the study. Outliers are data points that are significantly different from the rest of the data and may be due to measurement error, biological variation, or other factors. While outliers can be a inconvenience, they may also contain important information about the underlying population being studied, and removing them can lead to biased estimates and incorrect conclusions.

Therefore, it is generally recommended to report and analyze all data, including outliers, and to use appropriate statistical methods that can handle non-normal distributions or outliers, such as non-parametric tests or robust regression models. This can help ensure that the results of clinical studies are accurate, unbiased, and generalizable to the broader population of interest.

Because of the above mention we are not  going to filter outlier or remove them.

## Correlations between numerical variables 

```{r}
pairs(num_df)
```

```{r}
cor(num_df)
```

The table above show if the variable how a Pearson's correlation. We interpret the results that way:
* 1. Weak correlation: 0 < |r| < 0.3  
* 2. Moderate correlation:  0.3 < |r| < 0.7  
* 3. Strong correlation:   |r| > 0.7

Therefore, the  tabla  above shows that there are not moderate or strong correlation, just shows weak correlation between the numerical variables. 

For example `thalach`and `age` shows a weak negative linear association between them (-0.353)

# Creating a heatmap to verify the correlation result:
```{r}
heatmap(cor(num_df), cmap = colorRampPalette(c("blue", "white", "red"))(100))
```

### Interpretation: 
A heatmap is a graphical representation of data where the values of a matrix are represented as colors. Heatmaps are commonly used to display large datasets and provide a visual summary of the data.

As we can see above this heatmap represent the numerical variables that have a correlation. High correlation are represented by darker color and weak correlation are represented by lighter colors. By examining the heatmap, we could quickly identify patterns in the data and see that there is not strong correlation between the numerical variables.Therefore, with those variables it is quite likely that we are not going to have a multidisciplinary problems. 

## Visualization of  Normality distribution 
 
### For the entire variables

```{r}
 for (var in names(num_df)) {
  qqPlot(num_df[[var]], main = paste("Q-Q plot of", var))
}
```
But because we are going to analyze two sample populations divided by  the `HD` variable (0, 1). It is important to see if both samples follows a normal distribution, therefore, we are going to do that by analyzing the graphs. 

To check the normality distribution of the data, we are going to visualize a Q-Q Plot for every two samples
```{r}
# Normality of the two samples 
 for (var in names(num_df)) {
  group0 <- df[df$HD == 0,][,var]
  group1 <- df[df$HD == 1,][,var]
  par(mfrow=c(1,2)) # Set up 1x2 grid of plots
  qqPlot(group0, main = "QQ-Plot not having HD G-0")
  qqPlot(group1, main = "QQ-Plot having HD G-1")

 }
```

### Interpretation:

* For the variable `age` we can see that there are some parts at at the beginning and at the end that the points are shown in a diagonal line. However, in general the points are over the diagonal line. So we can conclude that the variable for the two groups follow a normal distribution.With the aforementioned variable we are going to decide if we are going to use the Anova test or a t-test taking into consideration the assumptions that goes with them. 
* For the variables : `trestbps`, `chol`, `chalach`, and `oldpeak` the points are very outside of the diagonal line so we can conclude that they don't follow a normal distribution, the one that is more evident is the oldpeak. For the variables mentioned before, we are going to use non parametric tests. We are going to use Main Whitney U test or Wilcoxon test, taking in consideration that they violated the assumption of normality distribution. 

# Data Visualization of the two groups 

```{r}
# Create a figure with subplots for each variable
par(mfrow=c(1, ncol(num_df)), mar=c(4, 4, 2, 1), oma=c(0, 0, 2, 0))
for (i in 1:(ncol(num_df))) {
  boxplot(num_df[,i] ~ df$HD, main=names(num_df)[i], ylab="Value")
}

# Add a title and adjust the margins
mtext("Boxplots for Five Numerical Variables by Group", outer=TRUE, cex=1.5)
```

# Analizing `Age` variable as important factor with a T- test. Two sided test.

```{r}
ggboxplot(df, x = "HD", y = "age", 
          color = "HD", palette = c("#00AFBB", "#E7B800"),
        ylab = "Weight", xlab = "HD")
```
#### Testing significance of the symmetric distribution of the variable  

* Ho: The two independent samples have identical average (expected) values
* Ha: The two independent samples do not have identical average (expected) values

Reject the null hypothesis is less or equal  0.05 significance level.

Assumptions:
    
* Data values must be independent. Measurements for one observation do not affect measurements for any other observation.
* Data in each group must be obtained via a random sample from the population.
* Data in each group are normally distributed.
* Data values are continuous.
* The variances for the two independent groups are equal.

### Test homogeneity of variances

* Ho: The variances of the groups are equal.
* Ha: At least one of the group variances is different from the others.
Significance level(alpha)= 0.05

#### Asumptions for this test:

* Independent observations 
* The test variable is quantitative, that is, not nominal or ordinal.

If any of these assumptions are violated, a different test should be used.

```{r}
leveneTest(age ~ HD, data = df)
```

### Interpretation:
* Since the p-value is less than 0.05, we  reject the null hypothesis and we can say  there is a  difference in the variances of the `age` of the two populations or group. So, we can conclude that the two groups have different variances.


```{r}
# Compute t-test
res <- t.test(age ~ HD, data = df, alternative = "two.sided", var.equal = FALSE)
res
```

#### Interpretation:
* Since the p-value is less than 0.05, we  reject the null hypothesis and we can say that there is a difference in the means between the two populations or group. Therefore, we can expect that the `age` has a statistical significant difference in the means in both groups( 0 and 1). So, we can conclude that this variable is a important factor for the model. 


We will create a function to apply the Wilcoxon test between all the the response variable and the explanatory variables:

## Wilcoxon Test- Non parametric test:

#### Testing significance of the symmetric distribution between the two groups. Two sided test.
* Ho: There is no difference in the distribution between the two populations or groups
* Ha: There is a significant difference in the distribution between the two population or groups
If p_value is less than 0.05 we reject the null hypothesis

#### Asumptions for this test:
It is an alternative measurement of unpaired t-test. A non parametric measurement

* Not normally distributed 
* The measure variable should be continuous or at least an order scale
* Two independent unpaired group

```{r}
test<- function(x) {
  wilcoxon <- wilcox.test(num_df[, x]~ as.numeric(df$HD), data = df)
  
  res <- data.frame('row' = 'HD'
                    , 'column' = colnames(num_df)[x]
                    ,  "p.value" =  wilcoxon$p.value
                    )
  return(res)
}
num <- do.call(rbind, lapply(seq_along(num_df)[-1], test))
num
```

#### Interpretation:
* Since the p-value is less than 0.05, we  reject the null hypothesis and we can say there is a  difference in the distribution between the two populations or group. Therefore, we can expect that the variable `trestbps`, `chol`, `thalach` and `oldpeak`have a statistical significant difference in the distribution in both groups(0 and 1). So, we can conclude that these variable are important factors for the model. 

Additional we are just going to try to do the same with `trestbps` just for practice even though the variable does not follow a normal distribution.

# Analizing `trestbps` variable as important factor 
```{r}
res1 <-t.test(trestbps ~ HD, data = df, alternative = "two.sided", var.equal = TRUE)
res1 
```
#### Interpretation:
* Since the p-value is less than 0.05, we  reject the null hypothesis and we can say there is a  difference in the means between the two populations or group. Therefore, we can expect that the `trestbps` has a statistical significant difference in the means in both groups( 0 and 1). So, we can conclude that this variable is a important factor for the model. 

# >>> SPLITTING THE DATA INTO TRAIN AND TEST
```{r}
set.seed(111)
data <-sort(sample(nrow(df), nrow(df)*.8))
train<-df[data,]
test<-df[-data,]
```

# >>> BUILDING MODELS:

# We will create our saturated model 
```{r}
model_1<- glm(HD ~.,family= binomial(link='logit'), data=train)
summary(model_1)
```

The variables that have p-value <0.05 in this saturated model are: `trestbps`, `age`, `sex`, `chol`, `thalach`, `exang` and  `oldpeak`. Those variables are important factor for the model.Next, considering that the p-values of some levels in the variables `cp`, `ca` and `thal` are less than 0.05, we will validate them one by one if they have any effect on the model and on the response variable:

```{r}
model_2<- glm(HD ~ age+sex+trestbps+chol+thalach+exang+oldpeak,family= binomial(link='logit'), data=train)
summary(model_2)
```
# --> Model 2 including the cp variable:
 
```{r}
model_cp<- glm(HD ~ age+trestbps+sex+chol+thalach+exang+oldpeak+cp,family= binomial(link='logit'), data=train)
summary(model_cp)
```

-  Testing if the cp variable is significant for the model

Ho: The base line model (reduced) is appropriate
Ha: The full model (including cp variable) is appropriate

```{r}
pchisq(model_2$deviance - model_cp$deviance, model_2$df.residual - model_cp$df.residual, lower.tail = FALSE)
```
Since the p value is less than 0.05, we have enough statistical evidence to reject the null hypothesis and we can conclude that the full model (including cp variable) is appropriate.

# --> Model 2 including the ca variable:

```{r}
model_ca<- glm(HD ~ age+sex+trestbps+chol++thalach+exang+oldpeak+ca,family= binomial(link='logit'), data=train)
summary(model_ca)
```

-  Testing if the ca variable is significant for the model

Ho: The base line model (reduced) is appropriate
Ha: The full model (including ca variable) is appropriate

```{r}
pchisq(model_2$deviance - model_ca$deviance, model_2$df.residual - model_ca$df.residual, lower.tail = FALSE)
```

Since the p value is less than 0.05, we have enough statistical evidence to reject the null hypothesis and we can conclude that the full model (including ca variable) is appropriate.

# --> Model 2 including the thal variable:

```{r}
model_thal<- glm(HD ~ age+sex+trestbps+chol+thalach+exang+oldpeak+thal,family= binomial(link='logit'), data=train)
summary(model_thal)
```

-  Testing if the thal variable is significant for the model

Ho: The base line model (reduced) is appropriate
Ha: The full model (including thal variable) is appropriate

```{r}
pchisq(model_2$deviance - model_thal$deviance, model_2$df.residual - model_thal$df.residual, lower.tail = FALSE)
```

Since the p value is less than 0.05, we have enough statistical evidence to reject the null hypothesis and we can conclude that the full model (including thal variable) is appropriate.

# --> Validating if our final base line model is better than the saturated model:

```{r}
simplemodel<- glm(HD ~ age+sex+cp+trestbps+chol+thalach+exang+oldpeak+ca+thal,family= binomial(link='logit'), data=train)
summary(simplemodel)
```

-  Testing if our simple model (reduced) is better than the saturated model:

Ho: The simple model (reduced) is appropriate
Ha: The saturated model is appropriate

```{r}
pchisq(simplemodel$deviance - model_1$deviance, simplemodel$df.residual - model_1$df.residual, lower.tail = FALSE)
```
Since the p value is greater than 0.05, we do not have enough statistical evidence to reject the null hypothesis and we can conclude that our base line model (reduced) is appropriate.

## Check Multicollinearity 
```{r}
vif(simplemodel)
```
Since the VIF is less than 5 for every variable we do not need to remove any variable from our model.

# Interaction term:

  - Creating the interaction between the variables age and trestbps:
  
```{r}
interact_plot(glm(formula=HD~age+sex+cp+trestbps+chol+thalach+exang+oldpeak+ca+thal+age:trestbps,family=binomial(link=logit),
        data=train),
        pred=age, modx=trestbps, geom ='line')
```

  - Creating the interaction between the variables chol and cp:

```{r}
interact_plot(glm(formula=HD~age+sex+cp+trestbps+chol+thalach+exang+oldpeak+ca+thal+chol:cp,family=binomial(link=logit),
        data=train),
        pred=chol, modx=cp, geom ='line')
```
* We explore every interaction possible with the variables in the dataset but we come up with these 2 interactions above that's showing a more significant interaction to the response variable HD.

* Based on the plots above, we will evaluate our simple model vs the model with the interactions between age and trestbps:


```{r}
model_with_interaction<- glm(HD ~ age+sex+cp+trestbps+chol+thalach+exang+oldpeak+ca+thal+age*trestbps,family= binomial(link='logit'), data=train)
summary(model_with_interaction)
```

-  Testing if our baseline model (reduced) is better than the model with interactions:

Ho: The reduce model is appropriate(without the interaction)
Ha:  The full model is appropriate(with interaction)

```{r}
pchisq(simplemodel$deviance - model_with_interaction$deviance,simplemodel$df.residual - model_with_interaction$df.residual,  lower.tail = FALSE)
```
* Since the p value is less than 0.05, we have enough statistical evidence to reject the null hypothesis and we can conclude that our model with interactions is appropriate.


# Model Performance Evaluation based on the 2 models (Simple Model vs Model with Interaction)
# ROC Curve, Accuracy, Specificity, Sensitivity
```{r}
#prob <- predict(baseline,newdata=test, type = "response")
#prob2 <- predict(model_with_interaction,newdata=test, type = "response")

prob <- predict(simplemodel,newdata=test)
prob2 <- predict(model_with_interaction,newdata=test)
```

```{r}
table <- table(prob,test$HD)
table2 <- table(prob2,test$HD)
```

```{r}
table(test$HD)
```


# Classification table for Simple Model
```{r}
# Assign each observation to a class based on the threshold value of 0.5
predicted_classes <- ifelse(prob >= 0.5, 1, 0)
actual_classes <- test$HD

# Convert the predicted and actual class vectors to factors
predicted_classes <- factor(predicted_classes, levels = c("0", "1"))
actual_classes <- factor(actual_classes, levels = c("0", "1"))

# Create a table of predicted versus actual classes
tab <- table(predicted_classes, actual_classes)

# Print the table
print(tab)
```

# Classification table for Model with interaction
```{r}
# Assign each observation to a class based on the threshold value of 0.5
predicted_classes2 <- ifelse(prob2 >= 0.5, 1, 0)
actual_classes <- test$HD

# Convert the predicted and actual class vectors to factors
predicted_classes2 <- factor(predicted_classes2, levels = c("0", "1"))
actual_classes2 <- factor(actual_classes, levels = c("0", "1"))

# Create a table of predicted versus actual classes
tab2 <- table(predicted_classes2, actual_classes)

# Print the table
print(tab2)
```

#Correct Classification
```{r}
sum1 <- sum(diag(tab))/sum(tab)
sum2 <- sum(diag(tab2))/sum(tab2)
print(c(CorrectClassification1 = sum1, CorrectClassification2 = sum2))
```

#Misclassification Rate
```{r}
m1 <- 1-sum(diag(tab))/sum(tab)
m2 <- 1-sum(diag(tab2))/sum(tab2)
print(c(MisClassification1 = m1, MisClassification2 = m2))
```

* From the results above, the second model which the model with interaction can correctly predict better to the individual outcome of having heart disease or not compared to the simple model, though both of these models are already good at predicting the heart disease, they only have slight difference in the correct classification percentage.
* Same goes with the misclassification, the model with interaction has lesser misclassification percentage compared to the simple model.

#Prediction Probability
```{r}
pred_val <- prediction(prob,test$HD)
pred_val2 <- prediction(prob2,test$HD)
```


### Accuracy values of both models (Simple and Model with interaction)
```{r}
accuracy <- mean(actual_classes == predicted_classes)
```

```{r}
accuracy2 <- mean(actual_classes2 == predicted_classes2)
print(c(BaselineModelAccuracy=accuracy,model_with_interactionAccuracy=accuracy2))
```
# Based on the result above, we can see that the model with interaction is showing higher accuracy percentage, therefore we can conclude that the model with interaction is predicting more accurately than the simple model, although the difference is just small.


#### Classification Report for the Simple Model 
### Identifying Specificity and Sensitivity
```{r}
TN <- tab[1,1]
FP <- tab[2,1]
FN <- tab[1,2]
TP <- tab[2,2]

tpr <- TP / (TP + FN)
fpr <- TN / (TN + FP)

print(c(Sensitivity =tpr, Specificity = fpr ))
```

#### Classification Report for Model with Interaction
### Identifying Accuracy and Cut-off,  Specificity and Sensitivity

```{r}
TN2 <- tab2[1,1]
FP2 <- tab2[2,1]
FN2 <- tab2[1,2]
TP2 <- tab2[2,2]

tpr2 <- TP2 / (TP2 + FN2)
fpr2 <- TN2 / (TN2 + FP2)

print(c(Sensitivity =tpr2, Specificity = fpr2 ))
```
# After comparing the sensitivity and specificity of the 2 models (simple model and with interaction), we can say that in terms of sensitivity, the model with interaction is predicting well compared to the simple model While in terms of specificity, they have the same prediciton level. The difference of performance between the two models is just very low.


# Area Under Curve (AUC) - Model 1 (Simple Model)
```{r}
auc <- performance(pred_val, "auc")@y.values
auc <- unlist(auc)
auc = round(auc,4)
```

# Area Under Curve (AUC) - Model 2 (Model with Interaction)
```{r}
auc2 <- performance(pred_val2, "auc")@y.values
auc2 <- unlist(auc2)
auc2 = round(auc2,4)
```

```{r}
print(c(AUC1 = auc,AUC2 = auc2))
```

### Based on the AUC values shown in the table above, it appears that AUC of the simple model has a higher value than AUC of the model with interaction, indicating that simple model is likely to have a better predictive power than the model with interaction, although the difference is not that huge.



# Receiver Operating Characteristic (ROC) Curve
```{r}
roc <- performance(pred_val, "tpr", "fpr")
roc2 <- performance(pred_val2, "tpr", "fpr")
```


```{r}
plot(roc,
     colorize=T,
     main = "ROC Curve",
     ylab = "Sensitivity",
     xlab = "1-Specificity")
plot(roc2, add=TRUE, col='red')
abline(a=0, b=1)
legend(.7,.5,legend=auc, title="Model1 AUC")
legend(.7,.2,legend=auc2, title="Model2 AUC")
```


### The ROC curve of model with interaction is higher than that of the simple model, indicating that model with interaction has a better performance in distinguishing between the risk of getting heart disease or not, with higher accuracy, sensitivity and specificity percentage value. Although their difference is not that high. We can still say that both model is a good predictor to the heart disease variable, thus, we can say that the simple model is enough to predict HD.

```{r}
# Perform the Hosmer-Lemeshow test to the simple model
hltest(simplemodel)
```

```{r}
# Perform the Hosmer-Lemeshow test to the model with interaction
hltest(model_with_interaction)
```


### Although after comparing 2 models (simple model and model with interaction), p-value is less than 0.05 so we can conclude that the model with interaction is a good model compared to the simple model, but we also see that the accuracy, sensitivity and specificity, and AUC is higher in the model with interaction, the difference in terms of prediction between is not that high. Both models are still giving reasonable results, thus we cannot conclude that the model with interaction is a better model.

### Also, based on the Hosmer and Lemeshow test, the simple model and the model with interaction terms have almost similar goodness-of-fit but the simple model has higher p-value that the model with interaction, it suggests that the interaction terms are not adding significant value to the model. This means that the simple model may be sufficient for predicting the outcome of interest, and adding interaction terms does not improve the performance in terms of predicting the risk of having heart disease or not. 
